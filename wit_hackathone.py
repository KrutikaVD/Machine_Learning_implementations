# -*- coding: utf-8 -*-
"""WiT_Hackathone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uRlOqTVE4kZlWj48StK8cB6P9BTlHCAN

Basic pip installation
"""

#!pip install simpletransformers
#!pip install emoji
#!pip install googletrans
#!pip install goslate
!pip install translate

"""# IMPORTS"""

import pandas as pd
import numpy as np
import chardet
import re
import sklearn
import itertools
import emoji
from simpletransformers.classification import ClassificationModel
import torch
from sklearn.impute import SimpleImputer
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

"""# IMPORT DATASET"""

from google.colab import files
data_to_load = files.upload()
#df_raw = pd.read_csv("C:\\Users\\Administrator\\Desktop\\hsd\\labeled_data.csv")

df_raw = pd.read_csv("labeled_data.csv", encoding='latin1')
#Try calling read_csv with encoding='latin1', encoding='iso-8859-1' or encoding='cp1252'

"""Target is defined as 1 & 0. ‘1’ indicates that it is a preventive information & ‘0’ indicates other wise"""

df_raw.head()

"""# Google Translate abstract data"""

from googletrans import Translator

translator = Translator()

"""take a copy of the dataset"""

df = df_raw.copy()

import goslate
from googletrans import Translator
from translate import Translator
translator= Translator(to_lang="en")
df['abstract']=[str(x) for x in df['abstract']]
df['abstract_en']=df['abstract'].map(lambda x:translator.translate (x))

df.head()

'''def trans_(text):
  dt1 = translator.detect(text)
  translated = translator.translate(text)
  return translated.text
for each_sentence in df['abstract']:
  trans_(each_sentence)'''

"""# NULL VALUE TREATMENT"""

df.isnull().sum()

"""Missing values treatment is performed using impute method i.e. replace with max entries for all missing values as the ratio of overall dataset w.r.t missing values is very minimal"""

df["Target"].fillna(1, inplace = True)

df.isnull().sum()

'''values = df.values
X = values[:,0:5]
y = values[:,5]
# define the imputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
# define the model
lda = LinearDiscriminantAnalysis()
# define the modeling pipeline
pipeline = Pipeline(steps=[('imputer', imputer),('model', lda)])
# define the cross validation procedure
kfold = KFold(n_splits=3, shuffle=True, random_state=1)
# evaluate the model
result = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')
# report the mean performance
print('Accuracy: %.3f' % result.mean())'''

df["Target"].dropna(inplace=True)
df.isnull().sum()

df.isnull().sum()

df.head()

df.describe()

"""Method 1: Tried to remove stop-words and lemmatization performed for abstract_en(encoded/translated english sentence)"""

from spacy.lang.en import English

nlp = English()

for text in df["abstract_en"]:

    # documents with linguistic annotations.
    my_doc = nlp(text)

    # Create list of word tokens
    token_list = []
    for token in my_doc:
        token_list.append(token.text)

    from spacy.lang.en.stop_words import STOP_WORDS

    # Create list of word tokens after removing stopwords
    filtered_sentence =[] 

    for word in token_list:
        lexeme = nlp.vocab[word]
        if lexeme.is_stop == False:
            filtered_sentence.append(word) 
    #print(token_list)
    
    for each_filtered_sentence in filtered_sentence:
        df["removed_stopwords"] = each_filtered_sentence

df.head()

"""Method 2: Tried to remove stop-words and lemmatization performed for abstract_en(encoded/translated english sentence)
Used this method
"""

import spacy
nlp = spacy.load('en')
stop_word_list = []
def stop_word(text):
    doc = nlp(text)
    for token in doc:
        print(token, token.lemma, token.lemma_)
        return token

'''for text in df["abstract_en"]:
    stop_word_list.append(stop_word(text))
print(stop_word_list)'''

df['removed_stopwords'] = df["abstract_en"].apply(stop_word)

"""Contraction dictionary prepared"""

contractions = { 
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he's": "he is",
"how'd": "how did",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'll": "i will",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'll": "it will",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"must've": "must have",
"mustn't": "must not",
"needn't": "need not",
"oughtn't": "ought not",
"shan't": "shall not",
"sha'n't": "shall not",
"she'd": "she would",
"she'll": "she will",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"that'd": "that would",
"that's": "that is",
"there'd": "there had",
"there's": "there is",
"they'd": "they would",
"they'll": "they will",
"they're": "they are",
"they've": "they have",
"wasn't": "was not",
"we'd": "we would",
"we'll": "we will",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"where'd": "where did",
"where's": "where is",
"who'll": "who will",
"who's": "who is",
"won't": "will not",
"wouldn't": "would not",
"you'd": "you would",
"you'll": "you will",
"you're": "you are",
"thx"   : "thanks"
}

def remove_contractions(text):
    return contractions[text.lower()] if text.lower() in contractions.keys() else text

"""Removal of contractions present in translated /encoded abstract data"""

df['abstract_en']=df['abstract_en'].apply(remove_contractions)
df.head()

"""Using Regular expression, identifying other data to be cleaned such as # Remove hashtag while keeping hashtag text, HTML special entities (e.g. &amp;), tickers, hyperlinks, whitespace (including new line characters), Basic Multilingual Plane (BMP) of Unicode, misspelling words, emoji, Mojibake etc"""

def clean_dataset(text):
    # Remove hashtag while keeping hashtag text
    text = re.sub(r'#','', text)
    # Remove HTML special entities (e.g. &amp;)
    text = re.sub(r'\&\w*;', '', text)
    # Remove tickers
    text = re.sub(r'\$\w*', '', text)
    # Remove hyperlinks
    text = re.sub(r'https?:\/\/.*\/\w*', '', text)
    # Remove whitespace (including new line characters)
    text = re.sub(r'\s\s+','', text)
    text = re.sub(r'[ ]{2, }',' ',text)
    # Remove URL, RT, mention(@)
    text=  re.sub(r'http(\S)+', '',text)
    text=  re.sub(r'http ...', '',text)
    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\S]+','',text)
    text=  re.sub(r'RT[ ]?@','',text)
    text = re.sub(r'@[\S]+','',text)
    # Remove words with 4 or fewer letters
    text = re.sub(r'\b\w{1,4}\b', '', text)
    #&, < and >
    text = re.sub(r'&amp;?', 'and',text)
    text = re.sub(r'&lt;','<',text)
    text = re.sub(r'&gt;','>',text)
    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:
    text= ''.join(c for c in text if c <= '\uFFFF') 
    text = text.strip()
    # Remove misspelling words
    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))
    # Remove emoji
    text = emoji.demojize(text)
    text = text.replace(":"," ")
    text = ' '.join(text.split()) 
    text = re.sub("([^\x00-\x7F])+"," ",text)
    # Remove Mojibake (also extra spaces)
    text = ' '.join(re.sub("[^\u4e00-\u9fa5\u0030-\u0039\u0041-\u005a\u0061-\u007a]", " ", text).split())
    return text

df['clean_data'] =df['abstract_en'].apply(clean_dataset)
df.head()

"""#Split the training and validation set"""

from sklearn.model_selection import train_test_split
'''X = df.iloc[:, [2, 3]].values
y = df.iloc[:, 4].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
#X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(df['tweet'], test_size=0.20, random_state=42)'''
X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(df['clean_data'], df['Target'], test_size=0.03, random_state=42)

train_df_clean = pd.concat([X_train_clean, y_train_clean], axis=1)
print("Shape of training data set: ", train_df_clean.shape)
print("View of data set: ", train_df_clean.head())

eval_df_clean = pd.concat([X_test_clean, y_test_clean], axis=1)
print("Shape of Eval data set: ", eval_df_clean.shape)

"""#BERT Model Training

Set up the train arguments
"""

train_args = {
    'evaluate_during_training': True,
    'logging_steps': 100,
    'num_train_epochs': 1,
    'evaluate_during_training_steps': 100,
    'save_eval_checkpoints': False,
    'train_batch_size': 32,
    'eval_batch_size': 64,
    'overwrite_output_dir': True,
    'fp16': False,
    'wandb_project': "visualization-demo"
}

model_BERT = ClassificationModel('bert', 'bert-base-cased', num_labels=2, use_cuda=False, from_tf=True, cuda_device=0, args=train_args)

"""Train the model"""

model_BERT.train_model(train_df_clean, eval_df=eval_df_clean)

result, model_outputs, wrong_predictions = model_BERT.eval_model(eval_df_clean, acc=sklearn.metrics.accuracy_score)

print(result)

model_Roberta = ClassificationModel('roberta', 'roberta-base', num_labels=2, use_cuda=True, cuda_device=0, args=train_args)

model_Roberta.train_model(train_df_clean, eval_df=eval_df_clean)

result, model_outputs, wrong_predictions = model_Roberta.eval_model(eval_df_clean, acc=sklearn.metrics.accuracy_score)

model_albert = ClassificationModel('albert', 'albert-base-v2', num_labels=2, use_cuda=True, cuda_device=0, args=train_args)

model_albert.train_model(train_df_clean, eval_df=eval_df_clean)

result, model_outputs, wrong_predictions = model_albert.eval_model(eval_df_clean, acc=sklearn.metrics.accuracy_score)